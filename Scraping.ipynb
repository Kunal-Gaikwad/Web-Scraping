{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \\[12\\]:\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    from psycopg2.extras import execute_batch\n",
    "    import psycopg2\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    def extract_data(page):\n",
    "        url = f'https://www.totaljobs.com/jobs/python/in-london?radius=30&page={page}'\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content , 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    joblist = []\n",
    "\n",
    "In \\[14\\]:\n",
    "\n",
    "    def transform(soup):      \n",
    "        divs = soup.find_all('div', class_ = \"Wrapper-sc-11673k2-0 eHVkAX\")\n",
    "        for item in divs:\n",
    "\n",
    "            title = item.find('h2').text\n",
    "            location = item.find('li', class_ = 'sc-fznXWL').text\n",
    "            description = item.find('div', class_ = 'sc-fzoYkl').text.strip()\n",
    "            salary = item.find('dl' , class_ = 'sc-fzoJMP').text\n",
    "            date = item.find('li' , class_ = 'sc-fznXWL jwFffy').text\n",
    "            company = item.find('div', class_ = 'sc-fzoiQi').text\n",
    "\n",
    "            tech_list = ['Python', 'Cloud', 'Rust','TensorFlow', 'PyTorch', 'Keras', 'OpenCV' ,'Natural Language Toolkit', 'NLTK','scikit learn',\n",
    "                  'Microsoft Cognitive Toolkit','Theano','Caffe', 'MXNet', 'H20.ai','IBM Watson Studio' , 'IBM Watson Assistant' ,'Google Cloud AutoML',\n",
    "                  'Azure','Kubernetes', 'c' , 'c++' , 'Apache Spark', 'Pyspark', 'ECMAscript','TypeScript','Gitlab', 'SQL', 'NoSQL' ,  'AWS,Data Lake', \n",
    "                  'Redshift','Glue', 'Celery', 'RabbitMQ', 'Airflow' , 'LINUX' ,'Java','Machine Learning','Go']\n",
    "            tech_list1 = [x.lower() for x in tech_list]\n",
    "            technology = [string for string in tech_list1 if string in description.lower()]\n",
    "\n",
    "            job ={\n",
    "                  'Title' : title,\n",
    "                  'Location' : location,\n",
    "                  'Description' : description,\n",
    "                  'Salary' : salary,\n",
    "                  'Date_Posted' : date,\n",
    "                  'Company' : company,\n",
    "                  'Technology': technology\n",
    "                 }\n",
    "\n",
    "            joblist.append(job)\n",
    "      \n",
    "        return joblist\n",
    "\n",
    "In \\[16\\]:\n",
    "\n",
    "    for i in [1,2]:\n",
    "        c = extract_data(i)\n",
    "        job_list = transform(c)\n",
    "\n",
    "In \\[19\\]:\n",
    "\n",
    "    conn = psycopg2.connect(host='localhost',database='postgres', user='postgres',password='Kunal',port='5432')\n",
    "\n",
    "    curs = conn.cursor()\n",
    "\n",
    "    curs.execute('''CREATE TABLE IF NOT EXISTS jobs ( \n",
    "                    Title TEXT NOT NULL, \n",
    "                    Location TEXT NOT NULL, \n",
    "                    Description TEXT NOT NULL, \n",
    "                    Salary TEXT NOT NULL, \n",
    "                    Date_Posted TEXT NOT NULL, \n",
    "                    Company TEXT NOT NULL,\n",
    "                    Technology TEXT NOT NULL)''')\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    query = '''INSERT INTO jobs VALUES (%(Title)s,\n",
    "                %(Location)s,\n",
    "                %(Description)s,\n",
    "                %(Salary)s,\n",
    "                %(Date_Posted)s,\n",
    "                %(Company)s,\n",
    "                %(Technology)s)'''\n",
    "\n",
    "In \\[25\\]:\n",
    "\n",
    "    execute_batch (curs, query,job_list)\n",
    "\n",
    "    conn.commit()\n",
    "    curs.close()\n",
    "    conn.close()"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
